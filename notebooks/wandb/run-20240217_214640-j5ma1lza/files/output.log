EDA.ipynb
Fooocus
Fooocus.ipynb
README.dataset.txt
README.roboflow.txt
Yolo.ipynb
Yolov7_Training.ipynb
_preview.png
clean.ipynb
cleaned_objects.csv
image_generation.ipynb
model.ipynb
object.csv
output.csv
pipeline.ipynb
position.csv
text.csv
wandb
yolov7
c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7
[34m[1mautoanchor: [39m[22mAnalyzing anchors... anchors/target = 3.72, Best Possible Recall (BPR) = 1.0000
YOLOR  2024-2-16 torch 2.2.0+cu121 CUDA:0 (NVIDIA GeForce GTX 1050, 4095.8125MB)
Namespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=False, cfg='cfg/training/yolov7.yaml', data='data.yaml', device='0', entity=None, epochs=100, evolve=False, exist_ok=False, freeze=[0], global_rank=-1, hyp='data/hyp.scratch.p5.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='yolov7', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs\\train\\yolov722', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, v5_metric=False, weights="''", workers=8, world_size=1)
[34m[1mtensorboard: [39m[22mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/
[34m[1mhyperparameters: [39m[22mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1
wandb: Currently logged in as: itsabel77. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...
wandb: \ Waiting for wandb.init()...
wandb: | Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\wandb\run-20240217_214734-9svbae9k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run yolov722
wandb:  View project at https://wandb.ai/itsabel77/YOLOR
wandb:  View run at https://wandb.ai/itsabel77/YOLOR/runs/9svbae9k
Overriding model.yaml nc=80 with nc=23
                 from  n    params  module                                  arguments
  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]
  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]
  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]
  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]
 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]
 12                -1  1         0  models.common.MP                        []
 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]
 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 16          [-1, -3]  1         0  models.common.Concat                    [1]
 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]
 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]
 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]
 25                -1  1         0  models.common.MP                        []
 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]
 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 29          [-1, -3]  1         0  models.common.Concat                    [1]
 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]
 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]
 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]
 38                -1  1         0  models.common.MP                        []
 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]
 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]
 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]
 42          [-1, -3]  1         0  models.common.Concat                    [1]
 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]
 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]
 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]
 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 55          [-1, -2]  1         0  models.common.Concat                    [1]
 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]
 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]
 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]
 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]
 67          [-1, -2]  1         0  models.common.Concat                    [1]
 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]
 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]
 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]
 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]
 76                -1  1         0  models.common.MP                        []
 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]
 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]
 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]
 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]
 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]
 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]
 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 89                -1  1         0  models.common.MP                        []
 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]
 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]
 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]
 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]
 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]
 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]
 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]
101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]
102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]
103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]
104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]
105   [102, 103, 104]  1    152824  models.yolo.IDetect                     [23, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]
c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\TensorShape.cpp:3550.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Model Summary: 415 layers, 37315224 parameters, 37315224 gradients, 105.5 GFLOPS
Scaled weight_decay = 0.0005
Optimizer groups: 95 .bias, 95 conv.weight, 98 other
[34m[1mtrain: [39m[22mScanning 'train.cache' images and labels... 345 found, 410 missing, 29 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755/755 [00:00<?, ?it/s]
[34m[1mtrain: [39m[22mScanning 'train.cache' images and labels... 345 found, 410 missing, 29 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755/755 [00:00<?, ?it/s]
[34m[1mval: [39m[22mScanning 'val.cache' images and labels... 39 found, 0 missing, 1 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<?, ?it/s]
[34m[1mval: [39m[22mScanning 'val.cache' images and labels... 39 found, 0 missing, 1 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<?, ?it/s]
Image sizes 640 train, 640 test
Using 8 dataloader workers
Logging results to runs\train\yolov722
Starting training for 100 epochs...
     Epoch   gpu_mem       box       obj       cls     total    labels  img_size
  0%|          | 0/34 [00:00<?, ?it/s]
  0%|          | 0/34 [07:19<?, ?it/s]
Traceback (most recent call last):
  File "train.py", line 616, in <module>
    train(hyp, opt, device, tb_writer)
  File "train.py", line 361, in train
    pred = model(imgs)  # forward
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\models\yolo.py", line 599, in forward
    return self.forward_once(x, profile)  # single-scale inference, train
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\models\yolo.py", line 625, in forward_once
    x = m(x)  # run
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\models\common.py", line 507, in forward
    return self.act(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\container.py", line 217, in forward
    input = module(input)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\batchnorm.py", line 175, in forward
    return F.batch_norm(
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\functional.py", line 2482, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.13 GiB is allocated by PyTorch, and 113.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
YOLOR  2024-2-16 torch 2.2.0+cu121 CUDA:0 (NVIDIA GeForce GTX 1050, 4095.8125MB)
Traceback (most recent call last):
  File "detect.py", line 196, in <module>
    detect()
  File "detect.py", line 34, in detect
    model = attempt_load(weights, map_location=device)  # load FP32 model
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\models\experimental.py", line 253, in attempt_load
    model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1688, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'Model' object has no attribute 'get'
Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.25, device='', exist_ok=False, img_size=640, iou_thres=0.45, name='exp', no_trace=False, nosave=False, project='runs/detect', save_conf=False, save_txt=False, source='_preview.png', update=False, view_img=False, weights=['runs/train/yolov722/weights/init.pt'])
YOLOR  2024-2-16 torch 2.2.0+cu121 CUDA:0 (NVIDIA GeForce GTX 1050, 4095.8125MB)
Traceback (most recent call last):
  File "detect.py", line 196, in <module>
    detect()
  File "detect.py", line 34, in detect
    model = attempt_load(weights, map_location=device)  # load FP32 model
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\models\experimental.py", line 253, in attempt_load
    model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1688, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'Model' object has no attribute 'get'
Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.25, device='', exist_ok=False, img_size=640, iou_thres=0.45, name='exp', no_trace=False, nosave=False, project='runs/detect', save_conf=False, save_txt=False, source='_preview.png', update=False, view_img=False, weights=['runs/train/yolov722/weights/init.pt'])
Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.25, device='', exist_ok=False, img_size=640, iou_thres=0.45, name='exp', no_trace=False, nosave=False, project='runs/detect', save_conf=False, save_txt=False, source='_preview.png', update=False, view_img=False, weights=['init.pt'])
YOLOR  2024-2-16 torch 2.2.0+cu121 CUDA:0 (NVIDIA GeForce GTX 1050, 4095.8125MB)
Traceback (most recent call last):
  File "detect.py", line 196, in <module>
    detect()
  File "detect.py", line 34, in detect
    model = attempt_load(weights, map_location=device)  # load FP32 model
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\models\experimental.py", line 253, in attempt_load
    model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\nn\modules\module.py", line 1688, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'Model' object has no attribute 'get'
YOLOR  2024-2-16 torch 2.2.0+cu121 CUDA:0 (NVIDIA GeForce GTX 1050, 4095.8125MB)
Namespace(adam=False, artifact_alias='latest', batch_size=5, bbox_interval=-1, bucket='', cache_images=False, cfg='cfg/training/yolov7.yaml', data='data.yaml', device='', entity=None, epochs=50, evolve=False, exist_ok=False, freeze=[0], global_rank=-1, hyp='data/hyp.scratch.p5.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='exp', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs\\train\\exp9', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=5, upload_dataset=False, v5_metric=False, weights="''", workers=8, world_size=1)
[34m[1mtensorboard: [39m[22mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/
[34m[1mhyperparameters: [39m[22mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1
wandb: Currently logged in as: itsabel77. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...
wandb: \ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\wandb\run-20240217_221241-qri2cva9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp9
wandb:  View project at https://wandb.ai/itsabel77/YOLOR
wandb:  View run at https://wandb.ai/itsabel77/YOLOR/runs/qri2cva9
Overriding model.yaml nc=80 with nc=23
                 from  n    params  module                                  arguments
  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]
  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]
  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]
  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]
 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]
 12                -1  1         0  models.common.MP                        []
 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]
 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 16          [-1, -3]  1         0  models.common.Concat                    [1]
 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]
 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]
 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]
 25                -1  1         0  models.common.MP                        []
 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]
 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 29          [-1, -3]  1         0  models.common.Concat                    [1]
 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]
 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]
 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]
 38                -1  1         0  models.common.MP                        []
 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]
 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]
 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]
 42          [-1, -3]  1         0  models.common.Concat                    [1]
 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]
 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]
 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]
 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 55          [-1, -2]  1         0  models.common.Concat                    [1]
 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]
 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]
 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]
 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]
 67          [-1, -2]  1         0  models.common.Concat                    [1]
 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]
 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]
 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]
 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]
 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]
 76                -1  1         0  models.common.MP                        []
 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]
 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]
 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]
 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]
 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]
 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]
 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]
 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]
 89                -1  1         0  models.common.MP                        []
 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]
 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]
 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]
 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]
 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]
 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]
 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]
100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]
101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]
102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]
103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]
104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]
105   [102, 103, 104]  1    152824  models.yolo.IDetect                     [23, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]
c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\TensorShape.cpp:3550.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Model Summary: 415 layers, 37315224 parameters, 37315224 gradients, 105.5 GFLOPS
Scaled weight_decay = 0.0005078125
Optimizer groups: 95 .bias, 95 conv.weight, 98 other
[34m[1mtrain: [39m[22mScanning 'train.cache' images and labels... 345 found, 410 missing, 29 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755/755 [00:00<?, ?it/s]
[34m[1mtrain: [39m[22mScanning 'train.cache' images and labels... 345 found, 410 missing, 29 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755/755 [00:00<?, ?it/s]
[34m[1mval: [39m[22mScanning 'val.cache' images and labels... 39 found, 0 missing, 1 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<?, ?it/s]
[34m[1mval: [39m[22mScanning 'val.cache' images and labels... 39 found, 0 missing, 1 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<?, ?it/s]
Image sizes 640 train, 640 test
Using 5 dataloader workers
Logging results to runs\train\exp9
Starting training for 50 epochs...
     Epoch   gpu_mem       box       obj       cls     total    labels  img_size
  0%|          | 0/108 [00:00<?, ?it/s]
      0/49      1.1G   0.08628   0.02155   0.05122    0.1591        44       640:   0%|          | 0/108 [05:10<?, ?it/s]
      0/49      1.1G   0.08628   0.02155   0.05122    0.1591        44       640:   1%|          | 1/108 [05:10<9:14:07, 310.73s/it]
      0/49     4.48G   0.08586   0.01988   0.05292    0.1587        18       640:   1%|          | 1/108 [05:25<9:14:07, 310.73s/it]
      0/49     4.48G   0.08586   0.01988   0.05292    0.1587        18       640:   2%|â–         | 2/108 [05:25<4:01:54, 136.93s/it]
      0/49     4.48G   0.08457   0.01929   0.05727    0.1611        15       640:   2%|â–         | 2/108 [05:35<4:01:54, 136.93s/it]
      0/49     4.48G   0.08457   0.01929   0.05727    0.1611        15       640:   3%|â–Ž         | 3/108 [05:35<2:18:08, 78.94s/it]
      0/49     4.49G   0.08437   0.01964   0.05563    0.1596        33       640:   3%|â–Ž         | 3/108 [05:46<2:18:08, 78.94s/it]
      0/49     4.49G   0.08437   0.01964   0.05563    0.1596        33       640:   4%|â–Ž         | 4/108 [05:46<1:30:22, 52.14s/it]
      0/49      4.5G   0.07843   0.01914   0.05181    0.1494        13       640:   4%|â–Ž         | 4/108 [05:58<1:30:22, 52.14s/it]
      0/49      4.5G   0.07843   0.01914   0.05181    0.1494        13       640:   5%|â–         | 5/108 [05:58<1:04:06, 37.35s/it]
      0/49      4.5G   0.08013   0.01934   0.05296    0.1524        28       640:   5%|â–         | 5/108 [06:08<1:04:06, 37.35s/it]
      0/49      4.5G   0.08013   0.01934   0.05296    0.1524        28       640:   6%|â–Œ         | 6/108 [06:08<48:08, 28.32s/it]
      0/49      4.5G   0.08197   0.01906   0.05422    0.1552        21       640:   6%|â–Œ         | 6/108 [06:19<48:08, 28.32s/it]
      0/49      4.5G   0.08197   0.01906   0.05422    0.1552        21       640:   6%|â–‹         | 7/108 [06:19<37:39, 22.37s/it]
      0/49      4.5G   0.08208   0.01938   0.05355     0.155        33       640:   6%|â–‹         | 7/108 [06:29<37:39, 22.37s/it]
      0/49      4.5G   0.08208   0.01938   0.05355     0.155        33       640:   7%|â–‹         | 8/108 [06:29<31:04, 18.65s/it]
      0/49      4.5G     0.083    0.0192   0.05303    0.1552        23       640:   7%|â–‹         | 8/108 [06:41<31:04, 18.65s/it]
      0/49      4.5G     0.083    0.0192   0.05303    0.1552        23       640:   8%|â–Š         | 9/108 [06:41<27:11, 16.48s/it]
      0/49      4.5G   0.08273   0.01925   0.05289    0.1549        26       640:   8%|â–Š         | 9/108 [06:50<27:11, 16.48s/it]
      0/49      4.5G   0.08273   0.01925   0.05289    0.1549        26       640:   9%|â–‰         | 10/108 [06:50<23:25, 14.34s/it]
      0/49      4.5G   0.07966   0.01901   0.05128    0.1499        11       640:   9%|â–‰         | 10/108 [07:02<23:25, 14.34s/it]
      0/49      4.5G   0.07966   0.01901   0.05128    0.1499        11       640:  10%|â–ˆ         | 11/108 [07:03<22:03, 13.65s/it]
      0/49      4.5G   0.08108   0.01876   0.05124    0.1511        19       640:  10%|â–ˆ         | 11/108 [07:12<22:03, 13.65s/it]
      0/49      4.5G   0.08108   0.01876   0.05124    0.1511        19       640:  11%|â–ˆ         | 12/108 [07:12<19:45, 12.35s/it]
      0/49      4.5G   0.08067   0.01871   0.05121    0.1506        17       640:  11%|â–ˆ         | 12/108 [07:22<19:45, 12.35s/it]
      0/49      4.5G   0.08067   0.01871   0.05121    0.1506        17       640:  12%|â–ˆâ–        | 13/108 [07:22<18:20, 11.59s/it]
      0/49      4.5G   0.08088    0.0186   0.05087    0.1503        16       640:  12%|â–ˆâ–        | 13/108 [07:30<18:20, 11.59s/it]
      0/49      4.5G   0.08088    0.0186   0.05087    0.1503        16       640:  13%|â–ˆâ–Ž        | 14/108 [07:30<16:35, 10.59s/it]
      0/49      4.5G   0.08098    0.0187   0.05065    0.1503        29       640:  13%|â–ˆâ–Ž        | 14/108 [07:40<16:35, 10.59s/it]
      0/49      4.5G   0.08098    0.0187   0.05065    0.1503        29       640:  14%|â–ˆâ–        | 15/108 [07:40<15:57, 10.30s/it]
      0/49      4.5G   0.08142   0.01864   0.05088    0.1509        23       640:  14%|â–ˆâ–        | 15/108 [07:48<15:57, 10.30s/it]
      0/49      4.5G   0.08142   0.01864   0.05088    0.1509        23       640:  15%|â–ˆâ–        | 16/108 [07:48<14:53,  9.72s/it]
      0/49      4.5G   0.08142   0.01877   0.05081     0.151        31       640:  15%|â–ˆâ–        | 16/108 [07:59<14:53,  9.72s/it]
      0/49      4.5G   0.08142   0.01877   0.05081     0.151        31       640:  16%|â–ˆâ–Œ        | 17/108 [07:59<15:07,  9.97s/it]
      0/49      4.5G   0.08195   0.01859   0.05087    0.1514        10       640:  16%|â–ˆâ–Œ        | 17/108 [08:08<15:07,  9.97s/it]
      0/49      4.5G   0.08195   0.01859   0.05087    0.1514        10       640:  17%|â–ˆâ–‹        | 18/108 [08:08<14:44,  9.83s/it]
      0/49      4.5G   0.08194   0.01866   0.05101    0.1516        31       640:  17%|â–ˆâ–‹        | 18/108 [08:18<14:44,  9.83s/it]
      0/49      4.5G   0.08194   0.01866   0.05101    0.1516        31       640:  18%|â–ˆâ–Š        | 19/108 [08:18<14:49, 10.00s/it]
      0/49      4.5G   0.08255   0.01854   0.05085    0.1519        21       640:  18%|â–ˆâ–Š        | 19/108 [08:29<14:49, 10.00s/it]
      0/49      4.5G   0.08255   0.01854   0.05085    0.1519        21       640:  19%|â–ˆâ–Š        | 20/108 [08:29<14:42, 10.02s/it]
      0/49      4.5G   0.08264   0.01852   0.05072    0.1519        25       640:  19%|â–ˆâ–Š        | 20/108 [08:39<14:42, 10.02s/it]
      0/49      4.5G   0.08264   0.01852   0.05072    0.1519        25       640:  19%|â–ˆâ–‰        | 21/108 [08:39<14:47, 10.20s/it]
      0/49      4.5G   0.08211   0.01847   0.05051    0.1511        15       640:  19%|â–ˆâ–‰        | 21/108 [08:53<14:47, 10.20s/it]
      0/49      4.5G   0.08211   0.01847   0.05051    0.1511        15       640:  20%|â–ˆâ–ˆ        | 22/108 [08:53<16:11, 11.29s/it]
      0/49     4.51G     0.082   0.01864   0.05056    0.1512        41       640:  20%|â–ˆâ–ˆ        | 22/108 [09:02<16:11, 11.29s/it]
      0/49     4.51G     0.082   0.01864   0.05056    0.1512        41       640:  21%|â–ˆâ–ˆâ–       | 23/108 [09:02<15:12, 10.73s/it]
      0/49     4.51G   0.08167   0.01873   0.05045    0.1508        27       640:  21%|â–ˆâ–ˆâ–       | 23/108 [09:12<15:12, 10.73s/it]
      0/49     4.51G   0.08167   0.01873   0.05045    0.1508        27       640:  22%|â–ˆâ–ˆâ–       | 24/108 [09:12<14:33, 10.40s/it]
      0/49     4.51G   0.08168   0.01872   0.05045    0.1509        31       640:  22%|â–ˆâ–ˆâ–       | 24/108 [09:21<14:33, 10.40s/it]
      0/49     4.51G   0.08168   0.01872   0.05045    0.1509        31       640:  23%|â–ˆâ–ˆâ–Ž       | 25/108 [09:21<13:38,  9.86s/it]
      0/49     4.51G   0.08161   0.01869   0.05046    0.1508        26       640:  23%|â–ˆâ–ˆâ–Ž       | 25/108 [09:36<13:38,  9.86s/it]
      0/49     4.51G   0.08161   0.01869   0.05046    0.1508        26       640:  24%|â–ˆâ–ˆâ–       | 26/108 [09:36<15:33, 11.38s/it]
      0/49     4.51G   0.08149   0.01864    0.0503    0.1504        18       640:  24%|â–ˆâ–ˆâ–       | 26/108 [09:45<15:33, 11.38s/it]
      0/49     4.51G   0.08149   0.01864    0.0503    0.1504        18       640:  25%|â–ˆâ–ˆâ–Œ       | 27/108 [09:45<14:41, 10.88s/it]
      0/49     4.51G   0.08132   0.01875   0.05025    0.1503        37       640:  25%|â–ˆâ–ˆâ–Œ       | 27/108 [09:54<14:41, 10.88s/it]
      0/49     4.51G   0.08132   0.01875   0.05025    0.1503        37       640:  26%|â–ˆâ–ˆâ–Œ       | 28/108 [09:54<13:35, 10.19s/it]
      0/49     4.51G   0.08105   0.01868   0.05022    0.1499        18       640:  26%|â–ˆâ–ˆâ–Œ       | 28/108 [10:05<13:35, 10.19s/it]
      0/49     4.51G   0.08105   0.01868   0.05022    0.1499        18       640:  27%|â–ˆâ–ˆâ–‹       | 29/108 [10:05<13:38, 10.36s/it]
      0/49     4.51G    0.0809   0.01875   0.05015    0.1498        31       640:  27%|â–ˆâ–ˆâ–‹       | 29/108 [10:14<13:38, 10.36s/it]
      0/49     4.51G    0.0809   0.01875   0.05015    0.1498        31       640:  28%|â–ˆâ–ˆâ–Š       | 30/108 [10:14<13:08, 10.11s/it]
      0/49     4.51G    0.0808   0.01879   0.05004    0.1496        27       640:  28%|â–ˆâ–ˆâ–Š       | 30/108 [10:25<13:08, 10.11s/it]
      0/49     4.51G    0.0808   0.01879   0.05004    0.1496        27       640:  29%|â–ˆâ–ˆâ–Š       | 31/108 [10:25<13:14, 10.32s/it]
      0/49     4.51G   0.08046    0.0188   0.04999    0.1493        23       640:  29%|â–ˆâ–ˆâ–Š       | 31/108 [10:36<13:14, 10.32s/it]
      0/49     4.51G   0.08046    0.0188   0.04999    0.1493        23       640:  30%|â–ˆâ–ˆâ–‰       | 32/108 [10:36<13:09, 10.39s/it]
      0/49     4.51G   0.08041   0.01867   0.05017    0.1492        10       640:  30%|â–ˆâ–ˆâ–‰       | 32/108 [10:47<13:09, 10.39s/it]
      0/49     4.51G   0.08041   0.01867   0.05017    0.1492        10       640:  31%|â–ˆâ–ˆâ–ˆ       | 33/108 [10:47<13:13, 10.58s/it]
      0/49     4.51G   0.08039   0.01866   0.05018    0.1492        27       640:  31%|â–ˆâ–ˆâ–ˆ       | 33/108 [11:00<13:13, 10.58s/it]
      0/49     4.51G   0.08039   0.01866   0.05018    0.1492        27       640:  31%|â–ˆâ–ˆâ–ˆâ–      | 34/108 [11:00<14:16, 11.58s/it]
      0/49     4.51G   0.08038   0.01866   0.05018    0.1492        27       640:  31%|â–ˆâ–ˆâ–ˆâ–      | 34/108 [11:10<14:16, 11.58s/it]
      0/49     4.51G   0.08038   0.01866   0.05018    0.1492        27       640:  32%|â–ˆâ–ˆâ–ˆâ–      | 35/108 [11:10<13:17, 10.92s/it]
      0/49     4.51G   0.08042   0.01862   0.05019    0.1492        22       640:  32%|â–ˆâ–ˆâ–ˆâ–      | 35/108 [11:20<13:17, 10.92s/it]
      0/49     4.51G   0.08042   0.01862   0.05019    0.1492        22       640:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 36/108 [11:20<12:39, 10.55s/it]
      0/49     4.51G   0.08061   0.01853   0.05025    0.1494        20       640:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 36/108 [11:28<12:39, 10.55s/it]
      0/49     4.51G   0.08061   0.01853   0.05025    0.1494        20       640:  34%|â–ˆâ–ˆâ–ˆâ–      | 37/108 [11:28<11:46,  9.95s/it]
      0/49     4.51G    0.0806   0.01846   0.05024    0.1493        20       640:  34%|â–ˆâ–ˆâ–ˆâ–      | 37/108 [11:43<11:46,  9.95s/it]
      0/49     4.51G    0.0806   0.01846   0.05024    0.1493        20       640:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 38/108 [11:43<13:24, 11.50s/it]
      0/49     4.51G   0.08033   0.01841   0.05025     0.149        18       640:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 38/108 [11:53<13:24, 11.50s/it]
      0/49     4.51G   0.08033   0.01841   0.05025     0.149        18       640:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 39/108 [11:53<12:37, 10.98s/it]
      0/49     4.51G   0.08035   0.01833   0.05018    0.1489        15       640:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 39/108 [12:01<12:37, 10.98s/it]
      0/49     4.51G   0.08035   0.01833   0.05018    0.1489        15       640:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 40/108 [12:01<11:35, 10.23s/it]
      0/49     4.51G   0.08043   0.01828   0.05011    0.1488        21       640:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 40/108 [12:12<11:35, 10.23s/it]
      0/49     4.51G   0.08043   0.01828   0.05011    0.1488        21       640:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 41/108 [12:12<11:35, 10.37s/it]
      0/49     4.51G   0.08054   0.01822   0.05015    0.1489        26       640:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 41/108 [12:22<11:35, 10.37s/it]
      0/49     4.51G   0.08054   0.01822   0.05015    0.1489        26       640:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 42/108 [12:22<11:11, 10.17s/it]
      0/49     4.51G   0.08039   0.01828   0.05008    0.1488        34       640:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 42/108 [12:32<11:11, 10.17s/it]
      0/49     4.51G   0.08039   0.01828   0.05008    0.1488        34       640:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 43/108 [12:32<11:10, 10.31s/it]
      0/49     4.51G   0.08029   0.01834   0.05001    0.1486        35       640:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 43/108 [12:42<11:10, 10.31s/it]
      0/49     4.51G   0.08029   0.01834   0.05001    0.1486        35       640:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 44/108 [12:42<10:54, 10.22s/it]
      0/49     4.53G    0.0801   0.01839   0.04995    0.1484        30       640:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 44/108 [12:54<10:54, 10.22s/it]
      0/49     4.53G    0.0801   0.01839   0.04995    0.1484        30       640:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/108 [12:54<11:16, 10.74s/it]
      0/49     4.53G   0.07998   0.01843   0.04989    0.1483        32       640:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/108 [13:06<11:16, 10.74s/it]
      0/49     4.53G   0.07998   0.01843   0.04989    0.1483        32       640:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 46/108 [13:06<11:22, 11.00s/it]
      0/49     4.53G   0.07978   0.01842   0.04983     0.148        20       640:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 46/108 [13:21<11:22, 11.00s/it]
      0/49     4.53G   0.07978   0.01842   0.04983     0.148        20       640:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 47/108 [13:21<12:26, 12.24s/it]
      0/49     4.53G   0.07962   0.01845   0.04978    0.1479        27       640:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 47/108 [13:36<12:26, 12.24s/it]
      0/49     4.53G   0.07962   0.01845   0.04978    0.1479        27       640:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 48/108 [13:36<13:07, 13.12s/it]
      0/49     4.53G   0.07958    0.0185   0.04971    0.1478        38       640:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 48/108 [13:49<13:07, 13.12s/it]
      0/49     4.53G   0.07958    0.0185   0.04971    0.1478        38       640:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 49/108 [13:49<12:39, 12.88s/it]
      0/49     4.53G   0.07952   0.01853    0.0497    0.1477        32       640:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 49/108 [13:59<12:39, 12.88s/it]
      0/49     4.53G   0.07952   0.01853    0.0497    0.1477        32       640:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 50/108 [13:59<11:49, 12.23s/it]
      0/49     4.53G   0.07952   0.01845    0.0497    0.1477        14       640:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 50/108 [14:10<11:49, 12.23s/it]
      0/49     4.53G   0.07952   0.01845    0.0497    0.1477        14       640:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 51/108 [14:10<11:13, 11.82s/it]
      0/49     4.53G   0.07923   0.01837   0.04977    0.1474        11       640:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 51/108 [14:21<11:13, 11.82s/it]
      0/49     4.53G   0.07923   0.01837   0.04977    0.1474        11       640:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 52/108 [14:21<10:45, 11.53s/it]
      0/49     4.53G   0.07911   0.01838   0.04971    0.1472        23       640:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 52/108 [14:36<10:45, 11.53s/it]
      0/49     4.53G   0.07911   0.01838   0.04971    0.1472        23       640:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 53/108 [14:36<11:30, 12.55s/it]
      0/49     4.53G   0.07901   0.01834   0.04969     0.147        22       640:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 53/108 [14:51<11:30, 12.55s/it]
      0/49     4.53G   0.07901   0.01834   0.04969     0.147        22       640:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 54/108 [14:51<11:57, 13.29s/it]
      0/49     4.53G   0.07888   0.01834   0.04963    0.1469        22       640:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 54/108 [15:03<11:57, 13.29s/it]
      0/49     4.53G   0.07888   0.01834   0.04963    0.1469        22       640:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 55/108 [15:03<11:23, 12.89s/it]
      0/49     4.53G   0.07875    0.0183   0.04957    0.1466        21       640:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 55/108 [15:13<11:23, 12.89s/it]
      0/49     4.53G   0.07875    0.0183   0.04957    0.1466        21       640:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/108 [15:13<10:22, 11.97s/it]
      0/49     4.53G   0.07858   0.01833   0.04953    0.1464        26       640:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/108 [15:23<10:22, 11.97s/it]
      0/49     4.53G   0.07858   0.01833   0.04953    0.1464        26       640:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 57/108 [15:23<09:49, 11.56s/it]
      0/49     4.53G   0.07857   0.01827    0.0497    0.1465        16       640:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 57/108 [15:35<09:49, 11.56s/it]
      0/49     4.53G   0.07857   0.01827    0.0497    0.1465        16       640:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 58/108 [15:35<09:31, 11.42s/it]
      0/49     4.53G   0.07853   0.01825   0.04972    0.1465        27       640:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 58/108 [15:49<09:31, 11.42s/it]
      0/49     4.53G   0.07853   0.01825   0.04972    0.1465        27       640:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 59/108 [15:49<10:09, 12.44s/it]
      0/49     4.53G   0.07845   0.01825   0.04976    0.1465        28       640:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 59/108 [16:05<10:09, 12.44s/it]
      0/49     4.53G   0.07845   0.01825   0.04976    0.1465        28       640:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 60/108 [16:05<10:37, 13.29s/it]
      0/49     4.53G   0.07827    0.0183   0.04977    0.1463        33       640:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 60/108 [16:17<10:37, 13.29s/it]
      0/49     4.53G   0.07827    0.0183   0.04977    0.1463        33       640:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 61/108 [16:17<10:08, 12.94s/it]
      0/49     4.53G   0.07821   0.01825   0.04978    0.1462        16       640:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 61/108 [16:27<10:08, 12.94s/it]
      0/49     4.53G   0.07821   0.01825   0.04978    0.1462        16       640:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 62/108 [16:27<09:18, 12.14s/it]
      0/49     4.53G    0.0782   0.01822   0.04973    0.1461        21       640:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 62/108 [16:38<09:18, 12.14s/it]
      0/49     4.53G    0.0782   0.01822   0.04973    0.1461        21       640:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 63/108 [16:38<08:46, 11.71s/it]
      0/49     4.53G   0.07812   0.01821    0.0497     0.146        29       640:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 63/108 [16:49<08:46, 11.71s/it]
      0/49     4.53G   0.07812   0.01821    0.0497     0.146        29       640:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 64/108 [16:49<08:26, 11.52s/it]
      0/49     4.53G   0.07752   0.01816   0.04943    0.1451        13       640:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 64/108 [17:04<08:26, 11.52s/it]
      0/49     4.53G   0.07752   0.01816   0.04943    0.1451        13       640:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 65/108 [17:04<09:01, 12.60s/it]
      0/49     4.53G   0.07744   0.01818   0.04938     0.145        30       640:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 65/108 [17:19<09:01, 12.60s/it]
      0/49     4.53G   0.07744   0.01818   0.04938     0.145        30       640:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 66/108 [17:19<09:25, 13.47s/it]
      0/49     4.53G   0.07697   0.01811   0.04913    0.1442        11       640:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 66/108 [17:31<09:25, 13.47s/it]
      0/49     4.53G   0.07697   0.01811   0.04913    0.1442        11       640:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 67/108 [17:31<08:51, 12.97s/it]
      0/49     4.53G   0.07692   0.01813   0.04912    0.1442        39       640:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 67/108 [17:42<08:51, 12.97s/it]
      0/49     4.53G   0.07692   0.01813   0.04912    0.1442        39       640:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 68/108 [17:42<08:08, 12.21s/it]
      0/49     4.53G   0.07677   0.01814   0.04911     0.144        26       640:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 68/108 [17:53<08:08, 12.21s/it]
      0/49     4.53G   0.07677   0.01814   0.04911     0.144        26       640:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 69/108 [17:53<07:44, 11.91s/it]
      0/49     4.53G   0.07687   0.01807   0.04907     0.144        15       640:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 69/108 [18:04<07:44, 11.91s/it]
      0/49     4.53G   0.07687   0.01807   0.04907     0.144        15       640:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 70/108 [18:04<07:24, 11.70s/it]
      0/49     4.53G    0.0768   0.01805   0.04905    0.1439        24       640:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 70/108 [18:19<07:24, 11.70s/it]
      0/49     4.53G    0.0768   0.01805   0.04905    0.1439        24       640:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 71/108 [18:19<07:51, 12.76s/it]
      0/49     4.53G   0.07676     0.018   0.04902    0.1438        16       640:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 71/108 [18:35<07:51, 12.76s/it]
      0/49     4.53G   0.07676     0.018   0.04902    0.1438        16       640:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 72/108 [18:35<08:12, 13.67s/it]
      0/49     4.53G   0.07668   0.01794   0.04906    0.1437        11       640:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 72/108 [18:47<08:12, 13.67s/it]
      0/49     4.53G   0.07668   0.01794   0.04906    0.1437        11       640:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 73/108 [18:47<07:36, 13.03s/it]
      0/49     4.53G   0.07663   0.01792   0.04903    0.1436        26       640:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 73/108 [18:56<07:36, 13.03s/it]
      0/49     4.53G   0.07663   0.01792   0.04903    0.1436        26       640:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 74/108 [18:56<06:45, 11.93s/it]
      0/49     4.53G   0.07662   0.01788   0.04902    0.1435        19       640:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 74/108 [19:06<06:45, 11.93s/it]
      0/49     4.53G   0.07662   0.01788   0.04902    0.1435        19       640:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 75/108 [19:06<06:14, 11.35s/it]
      0/49     4.53G   0.07652   0.01786     0.049    0.1434        21       640:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 75/108 [19:16<06:14, 11.35s/it]
      0/49     4.53G   0.07652   0.01786     0.049    0.1434        21       640:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 76/108 [19:16<05:50, 10.95s/it]
      0/49     4.53G   0.07646    0.0178   0.04895    0.1432        11       640:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 76/108 [19:30<05:50, 10.95s/it]
      0/49     4.53G   0.07646    0.0178   0.04895    0.1432        11       640:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 77/108 [19:30<06:08, 11.89s/it]
      0/49     4.53G    0.0764   0.01777   0.04893    0.1431        17       640:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 77/108 [19:44<06:08, 11.89s/it]
      0/49     4.53G    0.0764   0.01777   0.04893    0.1431        17       640:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 78/108 [19:44<06:15, 12.53s/it]
      0/49     4.53G   0.07625   0.01774    0.0489    0.1429        18       640:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 78/108 [19:55<06:15, 12.53s/it]
      0/49     4.53G   0.07625   0.01774    0.0489    0.1429        18       640:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 79/108 [19:55<05:48, 12.02s/it]
      0/49     4.53G   0.07617   0.01767   0.04887    0.1427         8       640:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 79/108 [20:05<05:48, 12.02s/it]
      0/49     4.53G   0.07617   0.01767   0.04887    0.1427         8       640:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 80/108 [20:05<05:17, 11.33s/it]
      0/49     4.53G   0.07609   0.01768   0.04886    0.1426        28       640:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 80/108 [20:15<05:17, 11.33s/it]
      0/49     4.53G   0.07609   0.01768   0.04886    0.1426        28       640:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 81/108 [20:15<04:59, 11.08s/it]
      0/49     4.53G   0.07605   0.01761   0.04887    0.1425        10       640:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 81/108 [20:26<04:59, 11.08s/it]
      0/49     4.53G   0.07605   0.01761   0.04887    0.1425        10       640:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 82/108 [20:26<04:46, 11.01s/it]
      0/49     4.53G   0.07591   0.01752   0.04868    0.1421         6       640:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 82/108 [20:40<04:46, 11.01s/it]
      0/49     4.53G   0.07591   0.01752   0.04868    0.1421         6       640:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 83/108 [20:40<04:57, 11.88s/it]
      0/49     4.53G   0.07554   0.01745   0.04847    0.1415         6       640:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 83/108 [20:53<04:57, 11.88s/it]
      0/49     4.53G   0.07554   0.01745   0.04847    0.1415         6       640:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 84/108 [20:53<04:57, 12.39s/it]
      0/49     4.53G   0.07544   0.01741   0.04847    0.1413        13       640:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 84/108 [21:04<04:57, 12.39s/it]
      0/49     4.53G   0.07544   0.01741   0.04847    0.1413        13       640:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 85/108 [21:04<04:34, 11.92s/it]
      0/49     4.53G   0.07518   0.01734   0.04826    0.1408         6       640:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 85/108 [21:14<04:34, 11.92s/it]
      0/49     4.53G   0.07518   0.01734   0.04826    0.1408         6       640:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 86/108 [21:14<04:05, 11.16s/it]
      0/49     4.53G   0.07513   0.01731   0.04824    0.1407        17       640:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 86/108 [21:24<04:05, 11.16s/it]
      0/49     4.53G   0.07513   0.01731   0.04824    0.1407        17       640:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 87/108 [21:24<03:48, 10.88s/it]
      0/49     4.53G   0.07503   0.01729    0.0482    0.1405        22       640:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 87/108 [21:34<03:48, 10.88s/it]
      0/49     4.53G   0.07503   0.01729    0.0482    0.1405        22       640:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 88/108 [21:34<03:34, 10.72s/it]
      0/49     4.53G   0.07488   0.01723   0.04822    0.1403         6       640:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 88/108 [21:48<03:34, 10.72s/it]
      0/49     4.53G   0.07488   0.01723   0.04822    0.1403         6       640:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 89/108 [21:48<03:42, 11.73s/it]
      0/49     4.53G   0.07479   0.01727   0.04821    0.1403        32       640:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 89/108 [22:03<03:42, 11.73s/it]
      0/49     4.53G   0.07479   0.01727   0.04821    0.1403        32       640:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 90/108 [22:03<03:46, 12.60s/it]
      0/49     4.53G   0.07467   0.01723   0.04821    0.1401        10       640:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 90/108 [22:14<03:46, 12.60s/it]
      0/49     4.53G   0.07467   0.01723   0.04821    0.1401        10       640:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 91/108 [22:14<03:27, 12.22s/it]
      0/49     4.53G   0.07461   0.01719    0.0482      0.14        11       640:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 91/108 [22:24<03:27, 12.22s/it]
      0/49     4.53G   0.07461   0.01719    0.0482      0.14        11       640:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 92/108 [22:24<03:02, 11.42s/it]
      0/49     4.53G   0.07455   0.01716   0.04817    0.1399        13       640:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 92/108 [22:34<03:02, 11.42s/it]
      0/49     4.53G   0.07455   0.01716   0.04817    0.1399        13       640:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 93/108 [22:34<02:45, 11.01s/it]
      0/49     4.53G   0.07438   0.01712   0.04818    0.1397        12       640:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 93/108 [22:44<02:45, 11.01s/it]
      0/49     4.53G   0.07438   0.01712   0.04818    0.1397        12       640:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 94/108 [22:44<02:30, 10.77s/it]
      0/49     4.53G   0.07438   0.01707   0.04835    0.1398        12       640:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 94/108 [22:58<02:30, 10.77s/it]
      0/49     4.53G   0.07438   0.01707   0.04835    0.1398        12       640:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 95/108 [22:58<02:32, 11.76s/it]
      0/49     4.53G   0.07432   0.01701   0.04836    0.1397         9       640:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 95/108 [23:12<02:32, 11.76s/it]
      0/49     4.53G   0.07432   0.01701   0.04836    0.1397         9       640:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 96/108 [23:12<02:29, 12.47s/it]
      0/49     4.53G   0.07408   0.01695   0.04817    0.1392         5       640:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 96/108 [23:23<02:29, 12.47s/it]
      0/49     4.53G   0.07408   0.01695   0.04817    0.1392         5       640:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 97/108 [23:23<02:12, 12.06s/it]
      0/49     4.53G   0.07406   0.01691   0.04822    0.1392        15       640:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 97/108 [23:33<02:12, 12.06s/it]
      0/49     4.53G   0.07406   0.01691   0.04822    0.1392        15       640:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 98/108 [23:33<01:52, 11.29s/it]
      0/49     4.53G   0.07402   0.01687   0.04819    0.1391        12       640:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 98/108 [23:43<01:52, 11.29s/it]
      0/49     4.53G   0.07402   0.01687   0.04819    0.1391        12       640:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 99/108 [23:43<01:39, 11.03s/it]
      0/49     4.53G   0.07395   0.01687   0.04819     0.139        24       640:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 99/108 [23:55<01:39, 11.03s/it]
      0/49     4.53G   0.07395   0.01687   0.04819     0.139        24       640:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 100/108 [23:55<01:30, 11.27s/it]
      0/49     4.53G   0.07392   0.01685   0.04817    0.1389        18       640:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 100/108 [24:11<01:30, 11.27s/it]
      0/49     4.53G   0.07392   0.01685   0.04817    0.1389        18       640:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 101/108 [24:11<01:27, 12.51s/it]
      0/49     4.53G   0.07383   0.01684   0.04815    0.1388        22       640:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 101/108 [24:24<01:27, 12.51s/it]
      0/49     4.53G   0.07383   0.01684   0.04815    0.1388        22       640:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 102/108 [24:24<01:17, 12.92s/it]
      0/49     4.53G   0.07382   0.01685   0.04814    0.1388        29       640:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 102/108 [24:36<01:17, 12.92s/it]
      0/49     4.53G   0.07382   0.01685   0.04814    0.1388        29       640:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 103/108 [24:36<01:02, 12.41s/it]
      0/49     4.53G   0.07375   0.01681   0.04811    0.1387        15       640:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 103/108 [24:45<01:02, 12.41s/it]
      0/49     4.53G   0.07375   0.01681   0.04811    0.1387        15       640:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 104/108 [24:45<00:45, 11.35s/it]
      0/49     4.53G   0.07388   0.01676    0.0481    0.1387        12       640:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 104/108 [24:54<00:45, 11.35s/it]
      0/49     4.53G   0.07388   0.01676    0.0481    0.1387        12       640:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 105/108 [24:54<00:32, 10.79s/it]
      0/49     4.53G   0.07388   0.01674   0.04809    0.1387        19       640:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 105/108 [25:04<00:32, 10.79s/it]
      0/49     4.53G   0.07388   0.01674   0.04809    0.1387        19       640:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 106/108 [25:04<00:20, 10.43s/it]
      0/49     4.53G   0.07382   0.01668   0.04807    0.1386         6       640:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 106/108 [25:17<00:20, 10.43s/it]
      0/49     4.53G   0.07382   0.01668   0.04807    0.1386         6       640:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 107/108 [25:17<00:11, 11.33s/it]
      0/49      1.2G   0.07386   0.01674   0.04805    0.1387        22       640:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 107/108 [25:47<00:11, 11.33s/it]
      0/49      1.2G   0.07386   0.01674   0.04805    0.1387        22       640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [25:47<00:00, 16.96s/it]
      0/49      1.2G   0.07386   0.01674   0.04805    0.1387        22       640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [25:47<00:00, 14.33s/it]
               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:   0%|          | 0/4 [00:00<?, ?it/s]
               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:15<00:47, 15.84s/it]
               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:34<01:43, 34.61s/it]
Traceback (most recent call last):
  File "train.py", line 616, in <module>
    train(hyp, opt, device, tb_writer)
  File "train.py", line 415, in train
    results, maps, times = test.test(data_dict,
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\test.py", line 125, in test
    out = non_max_suppression(out, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\notebooks\yolov7\utils\general.py", line 684, in non_max_suppression
    i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torchvision\ops\boxes.py", line 41, in nms
    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)
  File "c:\Users\Abel\Documents\10 Academy\week-10\Automated-Storyboard-Synthesis-for-Digital-Advertising\.venv\lib\site-packages\torch\_ops.py", line 755, in __call__
    return self._op(*args, **(kwargs or {}))
NotImplementedError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].
CPU: registered at C:\actions-runner\_work\vision\vision\pytorch\vision\torchvision\csrc\ops\cpu\nms_kernel.cpp:112 [kernel]
Meta: registered at /dev/null:440 [kernel]
QuantizedCPU: registered at C:\actions-runner\_work\vision\vision\pytorch\vision\torchvision\csrc\ops\quantized\cpu\qnms_kernel.cpp:124 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:154 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:498 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:324 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\NegateFallback.cpp:19 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:86 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:53 [backend fallback]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:57 [backend fallback]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:65 [backend fallback]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:69 [backend fallback]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:77 [backend fallback]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:61 [backend fallback]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:90 [backend fallback]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:73 [backend fallback]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:81 [backend fallback]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\autograd\TraceTypeManual.cpp:297 [backend fallback]
AutocastCPU: registered at C:\actions-runner\_work\vision\vision\pytorch\vision\torchvision\csrc\ops\autocast\nms_kernel.cpp:34 [kernel]
AutocastCUDA: registered at C:\actions-runner\_work\vision\vision\pytorch\vision\torchvision\csrc\ops\autocast\nms_kernel.cpp:27 [kernel]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:720 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:746 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:28 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:203 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:162 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:494 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:166 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:158 [backend fallback]
